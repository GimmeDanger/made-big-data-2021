{
  "metadata": {
    "name": "Tf-Idf",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df \u003d spark.read\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .option(\"sep\", \",\")\n    .csv(\"/opt/zeppelin/notebook/tripadvisor_hotel_reviews.csv\")\n\ndf.head"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// привести все к одному регистру\nval df_lower \u003d df\n    .withColumn(\"Review\", lower(col(\"Review\")))\n    .select(\"Review\")\ndf_lower.head"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// удалить все спецсимволы (кроме \\\\s и латинский букв)\nval df_clear \u003d df_lower\n    .withColumn(\"Review\", regexp_replace(col(\"Review\"), \"[^a-z ]\", \"\"))\ndf_clear.head"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// разбить предложение на массивы слов\nval df_words \u003d df_clear\n    .withColumn(\"Review\", trim(col(\"Review\")))\n    .withColumn(\"Review\", split(col(\"Review\"), \"\\\\s+\"))\ndf_words.head"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// развернуть массивы слов отдельные строки с идентификатором отзыва\nval df_words_counted \u003d df_words\n    .withColumn(\"ReviewId\", monotonically_increasing_id())\n    .withColumn(\"Word\", explode(col(\"Review\")))\n    .select(\"ReviewId\", \"Word\")\ndf_words_counted.show(200)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// посчитать частоту слова в рамках каждого документа: tf\n\nimport org.apache.spark.sql.expressions.Window\n\nval revieIdWordWindow \u003d Window.partitionBy(\"Reviewid\", \"Word\")\nval revieIdWindow \u003d Window.partitionBy(\"Reviewid\")\nval wordWindow \u003d Window.partitionBy(\"Word\")\n\nval df_calculated_tf \u003d df_words_counted\n    .withColumn(\"FD\", count(col(\"Word\")).over(revieIdWordWindow))\n    .dropDuplicates(\"ReviewId\", \"Word\", \"FD\")\n    .withColumn(\"SFD\", count(col(\"ReviewId\")).over(revieIdWindow))\n    .withColumn(\"TF\", col(\"FD\") / col(\"SFD\"))\n    .orderBy(\"ReviewId\")\n\ndf_calculated_tf.show(300)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// посчитать количество документов со словом, взять только 100 самых встречаемых\n\nval DN \u003d df.count()\nval df_calculated_idf \u003d df_calculated_tf\n    .select(\"ReviewId\", \"Word\")\n    .withColumn(\"DF\", count(col(\"Word\")).over(wordWindow))\n    .select(\"Word\", \"DF\")\n    .dropDuplicates(\"Word\")\n    .orderBy(desc(\"DF\"))\n    .limit(100)\n    .withColumn(\"DN\", lit(DN))\n    .withColumn(\"IDF\", log(col(\"DN\") / col(\"DF\")))\n    \n    \ndf_calculated_idf.show(100)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// cджойнить две полученные таблички и посчитать Tf-Idf (только для слов из предыдущего пункта)\nval tf_final \u003d df_calculated_tf\n    .select(\"ReviewId\", \"Word\", \"TF\")\n    .dropDuplicates()\n\nval idf_final \u003d df_calculated_idf\n    .select(\"Word\", \"IDF\")\n    .dropDuplicates()\n    \nval tf_idf \u003d tf_final\n    .join(idf_final, Seq(\"Word\"), \"inner\")\n    .withColumn(\"TFIDF\", col(\"TF\") * col(\"IDF\"))\n    .select(\"ReviewId\", \"Word\", \"TFIDF\")\ntf_idf.show(100)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// запайвотить табличку\n\nimport org.apache.spark.sql._\n\nval tf_idf_final \u003d tf_idf\n    .groupBy(\"ReviewId\")\n    .pivot(\"Word\")\n    .max(\"TFIDF\")\n    .na.fill(0)\n\ntf_idf_final\n    .select(tf_idf_final.columns.take(15).map(col): _*)\n    .show(5, 10)"
    }
  ]
}